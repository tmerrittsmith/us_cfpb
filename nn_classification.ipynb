{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Text Classification Playground\n",
    "\n",
    "In this notebook, I'm experimenting with text classification on the [US CFPB](https://www.kaggle.com/cfpb/us-consumer-finance-complaints) dataset. The models I tried are:\n",
    "\n",
    "- 1-layer LSTM + Dense Layer\n",
    "- 1-layer LSTM with meta-data features \n",
    "- 1-layer LSTM with glove embeddings and meta-data features\n",
    "- 1-D convnet with glove embeddings\n",
    "- 2-layer bidirectional LSTM with glove embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from tensorflow.keras.layers import Concatenate, Bidirectional\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('consumer_complaints.csv')\n",
    "df.columns = [c.replace(' ','_').lower().replace('-','').replace('?','') for c in df.columns]\n",
    "relief_tags = ['Closed with non-monetary relief',\n",
    "               'Closed with monetary relief','Closed with relief',\n",
    "                              ]\n",
    "df['relief_received'] = df.company_response_to_consumer.apply(\n",
    "                                   lambda x: 1 if x in relief_tags else 0)\n",
    "\n",
    "df_text = df.copy()\n",
    "df_text.dropna(axis=0,subset=['consumer_complaint_narrative'], inplace=True)\n",
    "df_text.reset_index(inplace=True)\n",
    "X = df_text['consumer_complaint_narrative'].apply(lambda x:x.lower())\n",
    "y = df_text.relief_received\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 20000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41213 unique tokens.\n",
      "Shape of data tensor: (39994, 500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train_text = tokenizer.texts_to_sequences(X_train.values)\n",
    "X_train_text = pad_sequences(X_train_text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_test_text = tokenizer.texts_to_sequences(X_test.values)\n",
    "X_test_text = pad_sequences(X_test_text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_train_text.shape)\n",
    "\n",
    "y_binary_train = to_categorical(y_train)\n",
    "y_binary_test = to_categorical(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,042,370\n",
      "Trainable params: 2,042,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 12s 83ms/step - loss: 0.5073 - acc: 0.8168 - val_loss: 0.4645 - val_acc: 0.8202\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 11s 80ms/step - loss: 0.4455 - acc: 0.8217 - val_loss: 0.4458 - val_acc: 0.8200\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 11s 79ms/step - loss: 0.4088 - acc: 0.8236 - val_loss: 0.4345 - val_acc: 0.8207\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 11s 80ms/step - loss: 0.3798 - acc: 0.8301 - val_loss: 0.4394 - val_acc: 0.8152\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 11s 79ms/step - loss: 0.3540 - acc: 0.8400 - val_loss: 0.4532 - val_acc: 0.8095\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 11s 80ms/step - loss: 0.3328 - acc: 0.8496 - val_loss: 0.4635 - val_acc: 0.8067\n"
     ]
    }
   ],
   "source": [
    "## basic lstm\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train_text.shape[1]))\n",
    "model.add(LSTM(64, dropout=0.2 ))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "opt = Adam(learning_rate=3e-4)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=opt, metrics=['acc'])\n",
    "print(model.summary())\n",
    "epochs = 10\n",
    "batch_size = 256 \n",
    "\n",
    "history = model.fit(X_train_text, y_binary_train, epochs=epochs,\n",
    "#                     class_weight={1:2,0:1}, # add some class weight to improve the recall\n",
    "                    batch_size=batch_size,validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_text.columns\n",
    "meta_columns = ['product','subproduct','issue','subissue','timely_response','consumer_disputed']\n",
    "meta_one_hot_encoded = pd.get_dummies(df_text[meta_columns])\n",
    "meta_ordinal_encoded = OrdinalEncoder().fit_transform(df_text[meta_columns].fillna('none'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_ordinal_encoded_train = meta_ordinal_encoded[X_train.index]\n",
    "meta_ordinal_encoded_test = meta_ordinal_encoded[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp_input = Input(name='text',shape=(X_train_text.shape[1],)) \n",
    "meta_input = Input(name='meta',shape=(meta_ordinal_encoded_train.shape[1],))\n",
    "\n",
    "emb = Embedding(output_dim=EMBEDDING_DIM, input_dim=MAX_NB_WORDS, input_length=X_train_text.shape[1])(nlp_input) \n",
    "nlp_out = Bidirectional(LSTM(128))(emb) \n",
    "concat = keras.layers.concatenate([nlp_out, meta_input]) \n",
    "classifier = Dense(32, activation='relu')(concat) \n",
    "output = Dense(2, activation='softmax')(classifier) \n",
    "model = Model(inputs=[nlp_input , meta_input], outputs=[output])\n",
    "\n",
    "opt = Adam(learning_rate=3e-4)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 100)     2000000     text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 256)          234496      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "meta (InputLayer)               [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 262)          0           bidirectional[0][0]              \n",
      "                                                                 meta[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           8416        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            66          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,242,978\n",
      "Trainable params: 2,242,978\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 26s 185ms/step - loss: 0.9736 - acc: 0.7716 - val_loss: 0.4641 - val_acc: 0.8150\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 25s 180ms/step - loss: 0.4602 - acc: 0.8206 - val_loss: 0.4563 - val_acc: 0.8202\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 25s 181ms/step - loss: 0.4469 - acc: 0.8210 - val_loss: 0.4418 - val_acc: 0.8195\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 26s 181ms/step - loss: 0.4130 - acc: 0.8277 - val_loss: 0.4431 - val_acc: 0.8207\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 26s 181ms/step - loss: 0.3721 - acc: 0.8397 - val_loss: 0.4638 - val_acc: 0.8190\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 25s 180ms/step - loss: 0.3382 - acc: 0.8534 - val_loss: 0.4922 - val_acc: 0.8152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2527513bd30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    {\"text\": X_train_text, \"meta\": meta_ordinal_encoded_train},\n",
    "    y_binary_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,validation_split=0.1,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90    131030\n",
      "           1       0.48      0.14      0.22     28946\n",
      "\n",
      "    accuracy                           0.82    159976\n",
      "   macro avg       0.66      0.55      0.56    159976\n",
      "weighted avg       0.77      0.82      0.77    159976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, \n",
    "                            model.predict({'text':X_test_text, \n",
    "                                           'meta':meta_ordinal_encoded_test}).argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "## Glove\n",
    "import os\n",
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "GLOVE_DIR = r\"GLOVE DIRECTORY\"\n",
    "\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39767 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "sequences = tokenizer.texts_to_sequences(X_train.values)\n",
    "train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.values)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "# embedded_sequences = embedding_layer(nlp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta_ordinal_encoded_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a8a1b0e4e985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnlp_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmeta_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'meta'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_ordinal_encoded_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m embedding_layer = Embedding(len(word_index) + 1,\n\u001b[0;32m      5\u001b[0m                             \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'meta_ordinal_encoded_train' is not defined"
     ]
    }
   ],
   "source": [
    "nlp_input = Input(name='text',shape=(None,)) \n",
    "meta_input = Input(name='meta',shape=(meta_ordinal_encoded_train.shape[1],))\n",
    "nlp_out = Bidirectional(LSTM(64))(embedded_sequences) \n",
    "concat = keras.layers.concatenate([nlp_out, meta_input]) \n",
    "classifier = Dense(32, activation='relu')(concat) \n",
    "output = Dense(2, activation='softmax')(classifier) \n",
    "model = Model(inputs=[nlp_input , meta_input], outputs=[output])\n",
    "\n",
    "opt = Adam(learning_rate=3e-4)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                   optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 1.6969 - acc: 0.7707\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4581 - acc: 0.8208\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4506 - acc: 0.8211\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4492 - acc: 0.8210\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4449 - acc: 0.8210\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4448 - acc: 0.8206\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4389 - acc: 0.8216\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4357 - acc: 0.8224\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4324 - acc: 0.8224\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.4282 - acc: 0.8231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x253e9285070>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=10\n",
    "batch_size=128\n",
    "model.fit(\n",
    "    {\"text\": train_data, \"meta\": meta_ordinal_encoded_train},\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    #validation_split=0.1,\n",
    "#     callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89    131030\n",
      "           1       0.43      0.24      0.31     28946\n",
      "\n",
      "    accuracy                           0.81    159976\n",
      "   macro avg       0.64      0.59      0.60    159976\n",
      "weighted avg       0.77      0.81      0.78    159976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, \n",
    "                            model.predict({'text':test_data, \n",
    "                                           'meta':meta_ordinal_encoded_test}).argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D CNN\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(2, activation=\"softmax\")(x)\n",
    "model = Model(int_sequences_input, preds)\n",
    "\n",
    "opt = Adam(learning_rate=3e-4)\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "240/240 [==============================] - 7s 27ms/step - loss: 0.4792 - acc: 0.8207 - val_loss: 0.4565 - val_acc: 0.8202\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.4543 - acc: 0.8216 - val_loss: 0.4394 - val_acc: 0.8200\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.4416 - acc: 0.8216 - val_loss: 0.4408 - val_acc: 0.8207\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.4254 - acc: 0.8230 - val_loss: 0.4378 - val_acc: 0.8202\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.4068 - acc: 0.8271 - val_loss: 0.4368 - val_acc: 0.8215\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.3797 - acc: 0.8363 - val_loss: 0.4785 - val_acc: 0.7990\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.3391 - acc: 0.8532 - val_loss: 0.4572 - val_acc: 0.8215\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.2765 - acc: 0.8841 - val_loss: 0.5159 - val_acc: 0.8075\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.2124 - acc: 0.9145 - val_loss: 0.5975 - val_acc: 0.7442\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.1488 - acc: 0.9451 - val_loss: 0.7391 - val_acc: 0.8083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25455aa7910>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, keras.utils.to_categorical(y_train), epochs=epochs,\n",
    "#                     class_weight={1:5,0:1}, # add some class weight to improve the recall\n",
    "                    batch_size=150,validation_split=0.1,\n",
    "#                     callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89    131030\n",
      "           1       0.40      0.13      0.20     28946\n",
      "\n",
      "    accuracy                           0.81    159976\n",
      "   macro avg       0.61      0.54      0.55    159976\n",
      "weighted avg       0.75      0.81      0.77    159976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, \n",
    "                            model.predict(test_data).argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.75\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(nlp_input)\n",
    "\n",
    "nlp_out = Bidirectional(LSTM(64, return_sequences=True))(embedded_sequences)\n",
    "nlp_out = Bidirectional(LSTM(64))(nlp_out)\n",
    "# classifier = Dense(32, activation='relu')(nlp_out) \n",
    "output = Dense(1, activation='sigmoid')(nlp_out) \n",
    "model = Model(nlp_input, output)\n",
    "\n",
    "opt = Adam(learning_rate=3e-4)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                   optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         3976800   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,160,225\n",
      "Trainable params: 183,425\n",
      "Non-trainable params: 3,976,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 24s 168ms/step - loss: 0.4798 - acc: 0.8186 - val_loss: 0.4681 - val_acc: 0.8227\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 22s 155ms/step - loss: 0.4682 - acc: 0.8201 - val_loss: 0.4662 - val_acc: 0.8227\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 22s 156ms/step - loss: 0.4630 - acc: 0.8201 - val_loss: 0.4606 - val_acc: 0.8227\n",
      "Epoch 4/10\n",
      "  4/141 [..............................] - ETA: 15s - loss: 0.4824 - acc: 0.8086"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, \n",
    "          y_train, \n",
    "          epochs=10,\n",
    "#                     class_weight={1:5,0:1}, # add some class weight to improve the recall\n",
    "                    batch_size=256,\n",
    "          validation_split=0.1,\n",
    "#                     callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf22",
   "language": "python",
   "name": "tf22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
